{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install --quiet --upgrade wrds pandas psycopg2-binary\n",
        "\n",
        "!pip install --quiet arch statsmodels openpyxl wrds\n"
      ],
      "metadata": {
        "id": "poel1jzlzDGm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import wrds, pandas as pd, numpy as np\n",
        "from google.colab import files\n",
        "import pandas as pd, numpy as np, os, datetime as dt\n",
        "from pathlib import Path"
      ],
      "metadata": {
        "id": "-ptU0P9-zIVm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "db = wrds.Connection()\n",
        "\n"
      ],
      "metadata": {
        "id": "ivai63AjEbi8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "files.upload()\n",
        "\n",
        "files.upload()"
      ],
      "metadata": {
        "id": "826cw1n4eRxH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Levels → Returns panel,\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# 0 ·  Paths\n",
        "# ---------------------------------------------------------------------------\n",
        "SECTOR_FILE = '/content/sp500_sector_indices.csv'\n",
        "CLOSE_FILE  = '/content/sp500_close_daily.csv'\n",
        "\n",
        "OUT_DIR = os.path.dirname(SECTOR_FILE) or '/content'\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# 1 ·  Read CSVs\n",
        "# ---------------------------------------------------------------------------\n",
        "sector_lvl = (pd.read_csv(SECTOR_FILE, parse_dates=['date'])\n",
        "                .set_index('date')\n",
        "                .sort_index())\n",
        "\n",
        "spx_close  = (pd.read_csv(CLOSE_FILE, parse_dates=['date'])\n",
        "                .set_index('date')\n",
        "                .sort_index())\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# 2 ·  Restrict to 1991-01-01 … 2024-11-01 (inclusive)\n",
        "# ---------------------------------------------------------------------------\n",
        "START = pd.Timestamp('1991-01-01')\n",
        "END   = pd.Timestamp('2024-11-01')\n",
        "\n",
        "sector_lvl = sector_lvl.loc[START:END]\n",
        "spx_close  = spx_close.loc[START:END]\n",
        "\n",
        "# align on union of trading days\n",
        "lvl_panel = sector_lvl.join(spx_close, how='outer')\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# 3 ·  Log-returns in percent\n",
        "# ---------------------------------------------------------------------------\n",
        "ret_panel = (np.log(lvl_panel).diff()\n",
        "             .mul(100)\n",
        "             .rename(columns={'sp500_close': 'SPX_logret'}))\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# 4 ·  Save\n",
        "# ---------------------------------------------------------------------------\n",
        "#jeg lod oprindeligt min data løbe ind i 2025 hvilket er for langt det rettede jeg men fik ikke ændret i navnene\n",
        "lvl_out = os.path.join(OUT_DIR, 'sp500_sector_indices_merged_1991_2025.csv')\n",
        "ret_out = os.path.join(OUT_DIR, 'sp500_sector_returns_panel_1991_2025.csv')\n",
        "\n",
        "lvl_panel.to_csv(lvl_out)\n",
        "ret_panel.to_csv(ret_out)\n",
        "\n"
      ],
      "metadata": {
        "id": "YhH69J-21dq4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Build ONE master CSV (levels + returns + RF)\n",
        "\n",
        "\n",
        "\n",
        "# ── paths (edit if needed) ──────────────────────────────────────────────────\n",
        "lvl_path    = '/content/sp500_sector_indices_merged_1991_2025.csv'\n",
        "ret_path    = '/content/sp500_sector_returns_panel_1991_2025.csv'\n",
        "master_path = '/content/sp500_sector_master_1991_2025.csv'\n",
        "\n",
        "# ── read levels & returns ───────────────────────────────────────────────────\n",
        "lvl_df = pd.read_csv(lvl_path, index_col=0, parse_dates=True)\n",
        "ret_df = pd.read_csv(ret_path, index_col=0, parse_dates=True)\n",
        "\n",
        "# ── add suffixes so names are unique ────────────────────────────────────────\n",
        "lvl_df = lvl_df.add_suffix('_lvl')                     # Energy_lvl, …\n",
        "\n",
        "ret_df = ret_df.rename(                                # Energy_ret, …\n",
        "    columns=lambda c: c if c == 'RF_log' else f'{c}_ret')\n",
        "\n",
        "# ── join on the date index ──────────────────────────────────────────────────\n",
        "master = lvl_df.join(ret_df, how='outer')              # no more collision\n",
        "\n",
        "# ── save & trigger download ────────────────────────────────────────────────\n",
        "master.to_csv(master_path)\n",
        "print(f\"Master CSV saved ➜ {master_path}\")\n",
        "print(\"Shape:\", master.shape)\n",
        "\n",
        "files.download(master_path)\n",
        "\n"
      ],
      "metadata": {
        "id": "KIVq2xr72Qoh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ------------------------------------------------------------\n",
        "# 1) download the two columns\n",
        "# ------------------------------------------------------------\n",
        "ff = db.get_table('ff', 'factors_daily', columns=['date', 'rf'])\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 2) robust date parsing\n",
        "#    - handles integers (19260701) AND strings ('1926-07-01')\n",
        "# ------------------------------------------------------------\n",
        "ff['date'] = pd.to_datetime(ff['date'], errors='coerce')\n",
        "\n",
        "# drop rows that failed to parse, sort, and keep one per day\n",
        "ff = (ff.dropna(subset=['date'])\n",
        "        .sort_values('date')\n",
        "        .drop_duplicates('date', keep='last')\n",
        "        .set_index('date'))\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 3) convert to continuously-compounded daily rate\n",
        "#    rf is in *percent* daily; divide by 100, then log(1+rf)\n",
        "# ------------------------------------------------------------\n",
        "ff['RF_log'] = np.log1p(ff['rf'] / 100)\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 4) align exactly to the trading-day index in your master panel\n",
        "#    (missing dates – normally the latest 4-5 weeks – stay NaN)\n",
        "# ------------------------------------------------------------\n",
        "df = df.copy()\n",
        "df['RF_log'] = ff['RF_log'].reindex(df.index)\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 5) save back to the same CSV (overwrite) and show a quick check\n",
        "# ------------------------------------------------------------\n",
        "df.to_csv(MASTER)\n",
        "non_na = df['RF_log'].notna().sum()\n",
        "\n"
      ],
      "metadata": {
        "id": "6Gku1SPm4qP9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "files.download(MASTER)\n"
      ],
      "metadata": {
        "id": "jV8wY3U_5v6x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "# ----- 1.  Define the calendar data ---------------------------------\n",
        "calendar = {\n",
        "    \"election\":   [1992, 1996, 2000, 2004, 2008, 2012, 2016, 2020],\n",
        "    \"call_date\":  [\n",
        "        \"1992-11-04\", \"1996-11-06\", \"2000-11-08\", \"2004-11-03\",\n",
        "        \"2008-11-05\", \"2012-11-07\", \"2016-11-09\", \"2020-11-05\"\n",
        "    ],\n",
        "    \"inaug_date\": [\n",
        "        \"1993-01-20\", \"1997-01-20\", \"2001-01-22\", \"2005-01-20\",\n",
        "        \"2009-01-20\", \"2013-01-22\", \"2017-01-20\", \"2021-01-20\"\n",
        "    ],\n",
        "    \"sectors\":    [\n",
        "        \"XLI|XLF|XLV\",  # 1992 Clinton\n",
        "        \"XLK|XLP|XLC\",  # 1996 Clinton\n",
        "        \"XLF|XLK|XLE\",  # 2000 Bush\n",
        "        \"XLE|XLI|XLF\",  # 2004 Bush\n",
        "        \"XLU|XLV|XLF\",  # 2008 Obama\n",
        "        \"XLE|XLU|XLV\",  # 2012 Obama\n",
        "        \"XLE|XLF|XLI\",  # 2016 Trump\n",
        "        \"XLU|XLI|XLV\"   # 2020 Biden\n",
        "    ]\n",
        "}\n",
        "\n",
        "# ----- 2.  Build a DataFrame ----------------------------------------\n",
        "cal_df = pd.DataFrame(calendar)\n",
        "\n",
        "# ----- 3.  Save to CSV -------------\n",
        "output_path = Path(\"election_calendar.csv\")\n",
        "cal_df.to_csv(output_path, index=False, encoding=\"utf-8\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gaVT67E4_UJz"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}